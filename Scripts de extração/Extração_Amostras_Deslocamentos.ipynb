{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bem-vindos\n",
        "Este notebook apresenta as etapas para a extração de dados utilizados em análises estatísticas (descritivas e inferenciais) para descrição Sociolinguística. Nele, são utilizados códigos para a extração de quatro fenômenos morfossintáticos variáveis do português brasileiro, a saber:\n",
        "\n",
        "a) Uso variável de artigo definido antes de possessivos pré-nominais;\n",
        "b) Variação nos pronomes pessoais de segunda pessoa do singular (2PS);\n",
        "c) Variação nos pronomes clíticos de 2PS; e\n",
        "d) Variação nos pronomes possessivos de 2PS.\n",
        "\n",
        "A extração é feito a partir de códigos provenientes spaCy 3.6 (HONIIBAL et al., 2020), biblioteca aberta de Processamento de Linguagem Natural (PLN), por meio da função Matcher (busca de tokens), que localiza o termo buscado, a partir de arquivos .txt gerados por meio de entrevistas sociolinguísticas transcritas, considerando os critérios de seleção elencados pelo pesquisador. Nesse sentido, os códigos de busca foram escritos considerando os seguintes critérios, que seguem a ordem disposta acima:\n",
        "\n",
        "f)\tPara o fenômeno em (a), apenas quando os possessivos precederem nomes;\n",
        "g)\tPara o fenômeno em (b), apenas quando as variantes estiverem em função de sujeito, foneticamente explícitas, precedendo ou sucedendo o verbo com o qual estabelece concordância.\n",
        "h)\tPara o fenômeno em (c), quando o clítico estiver em função dativa ou acusativa, em ênclise ou próclise; e\n",
        "i)\tPara o fenômeno em (d), quando os possessivos precederem e sucederem nomes.\n",
        "\n",
        "\n",
        "Por fim, por ser uma biblioteca em linguagem Python, após a extração dos dados pelo spaCy 3.6, usamos a função data.frame da biblioteca pandas para armazenar os dados em planilha para utilização em outras plataformas, como o R."
      ],
      "metadata": {
        "id": "uKDKyWPuWDdT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2qIxtlC5V2r"
      },
      "source": [
        "# 1. Montar o Google Drive\n",
        "Neste momento, precisamos ligar o Drive ao Notebook, de modo com que possamos acessar os arquivos do Drive aqui. Ao executar a função, uma janela será aberta para que você acesse a conta Google na qual você está usando o Google Colab, que é a mesma a qual você utilizará o Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YxbpvJPb4FdD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ebac7bb-ff48-4542-dfed-98eefc5396f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LmSRA1UjqAg"
      },
      "source": [
        "# 2. Instalar as bibliotecas\n",
        "Neste momento, são instaladas e importadas todas as bibliotecas e componentes necessários para funcionamento dos códigos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o5FkjndkjsYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "009207cd-c408-4d98-db93-9f2a66648cc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting spacy-lookups-data\n",
            "  Downloading spacy_lookups_data-1.0.5-py2.py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy-lookups-data) (71.0.4)\n",
            "Downloading spacy_lookups_data-1.0.5-py2.py3-none-any.whl (98.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spacy-lookups-data\n",
            "Successfully installed spacy-lookups-data-1.0.5\n",
            "Collecting pt-core-news-lg==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.7.0/pt_core_news_lg-3.7.0-py3-none-any.whl (568.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.2/568.2 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from pt-core-news-lg==3.7.0) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.9.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.23.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.1.2)\n",
            "Installing collected packages: pt-core-news-lg\n",
            "Successfully installed pt-core-news-lg-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: wasabi in /usr/local/lib/python3.10/dist-packages (1.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy\n",
        "\n",
        "!pip install -U spacy-lookups-data\n",
        "\n",
        "!python -m spacy download pt_core_news_lg\n",
        "\n",
        "!pip install pandas\n",
        "\n",
        "!pip install wasabi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "obP500rcr9dp"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('pt_core_news_lg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ygsElQJ5rmF"
      },
      "source": [
        "# 3. Carregar os arquivos .txt do Google Drive\n",
        "Para tanto, é necessário:\n",
        "- Criar uma pasta no Google Drive contendo os arquivos .txt que você deseja processar.\n",
        "- Determinar o caminho para a pasta no Google Drive. Por exemplo, se a pasta estiver localizada em \"Meu Drive/Minha Pasta\", o caminho será \"/content/drive/MyDrive/Minha Pasta\".\n",
        "- Execute o seguinte código para carregar os arquivos .txt.\n",
        "\n",
        "Aqui é importante que todos os arquivos possuam um padrão similar de identificação. Caso contrário, podem ocorrer erros na extração de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FXV_yzJzUAej"
      },
      "outputs": [],
      "source": [
        "nomes_arq = !ls '/content/drive/MyDrive/Entrevistas/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZQHYNtveZxB"
      },
      "source": [
        "# 4. Extração de informação dos arquivos\n",
        "O seguinte código é utilizado para a extração de informações presentes na própria entrevista, como nome do arquivo. Em nosso caso, o nome do arquivo possui todas as informações que desejamos, como informações sobre o falante (gênero, idade, deslocamento e tempo no curso) e sobre a amostra (São Cristóvão 2019 e São Cristóvão 2020)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75g1SGAKnBO0"
      },
      "source": [
        "Código para extrair informações dos nomes das entrevistas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YCZenVZInJ82"
      },
      "outputs": [],
      "source": [
        "def extrai_num_ent(nome_arquivo):\n",
        "    num_ent = nome_arquivo[0:65]\n",
        "    return num_ent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rzRtV_5e7Kw"
      },
      "source": [
        "# 5. Extração dos fenômenos\n",
        "A partir daqui, lidamos com a extração das ocorrências dos fenômenos, cada um dos fenômenos isoladamente. Para tanto, esta seção apresenta:\n",
        "- Código de busca e classificação\n",
        "- Armazenamento dos resultados em arquivo .xlsx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DX_AjPE2VCO"
      },
      "source": [
        "## 5.1 Determinantes possessivos antecedendo nomes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ysSAXqVEzRNd"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from wasabi import Printer\n",
        "import pandas as pd\n",
        "\n",
        "colunas = ['Numero_ent', 'Contexto anterior', 'Ocorrencia','Contexto Seguinte', 'Contexto']\n",
        "linhas = []\n",
        "\n",
        "nlp = spacy.load('pt_core_news_lg')\n",
        "\n",
        "nomes_arq = !ls '/content/drive/MyDrive/Entrevistas'\n",
        "\n",
        "for nome in nomes_arq:\n",
        "  nome_refatorado = nome.replace(\"'\", \"\").strip()\n",
        "  arq =f'/content/drive/MyDrive/Entrevistas/{nome_refatorado}'\n",
        "  corpus = open(arq).read()\n",
        "  doc = nlp(corpus)\n",
        "  numero_ent = extrai_num_ent(nome)\n",
        "\n",
        "  matcher = Matcher(vocab=nlp.vocab)\n",
        "  detso = [{'LEMMA': {'NOT_IN': ['meu','teu', 'seu', 'nosso']}},\n",
        "           {'POS': 'DET', 'MORPH':{'IS_SUPERSET': ['PronType=Prs']}},\n",
        "           {'POS': 'NOUN'}]\n",
        "\n",
        "  matcher.add('detso', [detso])\n",
        "  detrex = matcher(doc, as_spans=True)\n",
        "  matches = matcher(doc)\n",
        "\n",
        "  match = Printer()\n",
        "  for match_id, start, end in matches:\n",
        "    ocorrencia = doc[start:end]\n",
        "    contexto = doc[start-20:end+20]\n",
        "    contexto_parte1 = doc[start-20:start]\n",
        "    contexto_parte2 = doc[end: end+20]\n",
        "    linhas.append ([numero_ent, contexto_parte1, ocorrencia,contexto_parte2, contexto])\n",
        "\n",
        "dataframe = pd.DataFrame(linhas, columns = colunas)\n",
        "dataframe.to_excel(\"det.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPOu_ls22p6Z"
      },
      "source": [
        "## 5.2 Pronomes pessoais de 2PS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Zcvbt2q2tsr"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from wasabi import Printer\n",
        "import pandas as pd\n",
        "\n",
        "colunas = ['Numero_ent', 'Contexto anterior', 'Ocorrencia','Contexto Seguinte', 'Contexto']\n",
        "linhas = []\n",
        "\n",
        "nlp = spacy.load('pt_core_news_lg')\n",
        "\n",
        "nomes_arq = !ls '/content/drive/MyDrive/Entrevistas'\n",
        "\n",
        "for nome in nomes_arq:\n",
        "  nome_refatorado = nome.replace(\"'\", \"\").strip()\n",
        "  arq =f'/content/drive/MyDrive/Entrevistas/{nome_refatorado}'\n",
        "  corpus = open(arq).read()\n",
        "  doc = nlp(corpus)\n",
        "  numero_ent = extrai_num_ent(nome)\n",
        "  matcher = Matcher(vocab=nlp.vocab)\n",
        "  detso = [{'ORTH': {'IN': ['você', 'cê', 'tu']}}, {'POS': 'VERB', 'MORPH': {'IS_SUPERSET': ['VerbForm=Fin']}}]\n",
        "  inte = [{'ORTH': {'IN': ['você', 'cê', 'tu']}}, {}, {'POS': 'VERB', 'MORPH': {'IS_SUPERSET': ['VerbForm=Fin']}}] # com material interveniente\n",
        "\n",
        "  matcher.add('detso', [detso])\n",
        "  matcher.add('inte', [inte])\n",
        "  detrex = matcher(doc, as_spans=True)\n",
        "  matches = matcher(doc)\n",
        "  match = Printer()\n",
        "  for match_id, start, end in matches:\n",
        "        ocorrencia = doc[start:end]\n",
        "        contexto = doc[start-20:end+20]\n",
        "        contexto_parte1 = doc[start-20:start]\n",
        "        contexto_parte2 = doc[end: end+20]\n",
        "        linhas.append ([numero_ent, contexto_parte1, ocorrencia,contexto_parte2, contexto])\n",
        "\n",
        "dataframe = pd.DataFrame(linhas, columns = colunas)\n",
        "dataframe.to_excel(\"pro.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqj3Akdo6xzS"
      },
      "source": [
        "## 5.3 Clíticos de 2PS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ui4pl-Og6wXv"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from wasabi import Printer\n",
        "import pandas as pd\n",
        "\n",
        "colunas = ['Numero_ent', 'Contexto anterior', 'Ocorrencia','Contexto Seguinte', 'Contexto']\n",
        "linhas = []\n",
        "\n",
        "nlp = spacy.load('pt_core_news_lg')\n",
        "\n",
        "nomes_arq = !ls '/content/drive/MyDrive/Entrevistas'\n",
        "\n",
        "for nome in nomes_arq:\n",
        "  nome_refatorado = nome.replace(\"'\", \"\").strip()\n",
        "  arq =f'/content/drive/MyDrive/Entrevistas/{nome_refatorado}'\n",
        "  corpus = open(arq).read()\n",
        "  doc = nlp(corpus)\n",
        "  numero_ent = extrai_num_ent(nome)\n",
        "  matcher = Matcher(vocab=nlp.vocab)\n",
        "  matcher = Matcher(vocab=nlp.vocab)\n",
        "  detso = [{'ORTH': {'IN': ['te', 'lhe']}}, {'POS': 'VERB'}]\n",
        "  clitc= [{'POS': 'VERB'}, {'ORTH': {'IN': ['te', 'lhe']}}]\n",
        "  encli= [{'POS': 'VERB'}, {'IS_PUNCT': True}, {'ORTH': {'IN': ['te', 'lhe']}}]\n",
        "\n",
        "  matcher.add('detso', [detso])\n",
        "  matcher.add('clitc', [clitc])\n",
        "  matcher.add('encli', [encli])\n",
        "  detrex = matcher(doc, as_spans=True)\n",
        "  matches = matcher(doc)\n",
        "  match = Printer()\n",
        "  for match_id, start, end in matches:\n",
        "        ocorrencia = doc[start:end]\n",
        "        contexto = doc[start-20:end+20]\n",
        "        contexto_parte1 = doc[start-20:start]\n",
        "        contexto_parte2 = doc[end: end+20]\n",
        "        linhas.append ([numero_ent, contexto_parte1, ocorrencia,contexto_parte2, contexto])\n",
        "\n",
        "dataframe = pd.DataFrame(linhas, columns = colunas)\n",
        "dataframe.to_excel(\"cli.xlsx\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ecXiS-Z_MQM"
      },
      "source": [
        "## 5.4 Possessivos de 2PS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEiaDT219Yit"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from wasabi import Printer\n",
        "import pandas as pd\n",
        "\n",
        "colunas = ['Numero_ent', 'Contexto anterior', 'Ocorrencia','Contexto Seguinte', 'Contexto']\n",
        "linhas = []\n",
        "\n",
        "nlp = spacy.load('pt_core_news_lg')\n",
        "\n",
        "nomes_arq = !ls '/content/drive/MyDrive/Entrevistas'\n",
        "\n",
        "for nome in nomes_arq:\n",
        "  nome_refatorado = nome.replace(\"'\", \"\").strip()\n",
        "  arq =f'/content/drive/MyDrive/Entrevistas/{nome_refatorado}'\n",
        "  corpus = open(arq).read()\n",
        "  doc = nlp(corpus)\n",
        "  numero_ent = extrai_num_ent(nome)\n",
        "\n",
        "  matcher = Matcher(vocab=nlp.vocab)\n",
        "  pre = [{\"TEXT\": {\"IN\":[\"seu\", \"sua\", \"seus\", \"suas\", \"teu\", \"tua\", \"teus\", \"tuas\"]}}, {\"POS\": \"NOUN\"}]\n",
        "  pos = [{\"POS\": \"NOUN\"}, {\"TEXT\": {\"IN\":[\"seu\", \"sua\", \"seus\", \"suas\", \"teu\", \"tua\", \"teus\", \"tuas\"]}}]\n",
        "\n",
        "  matcher.add('pre', [pre])\n",
        "  matcher.add('pos', [pos])\n",
        "  detrex = matcher(doc, as_spans=True)\n",
        "  matches = matcher(doc)\n",
        "  match = Printer()\n",
        "  for match_id, start, end in matches:\n",
        "    ocorrencia = doc[start:end]\n",
        "    contexto = doc[start-20:end+20]\n",
        "    contexto_parte1 = doc[start-20:start]\n",
        "    contexto_parte2 = doc[end: end+20]\n",
        "    linhas.append ([numero_ent, contexto_parte1, ocorrencia,contexto_parte2, contexto])\n",
        "\n",
        "dataframe = pd.DataFrame(linhas, columns = colunas)\n",
        "dataframe.to_excel(\"pos.xlsx\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}